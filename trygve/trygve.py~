import sys
from mpl_toolkits.mplot3d import Axes3D
from imageio import imread
import matplotlib.pyplot as plt
from matplotlib import cm
from matplotlib.ticker import LinearLocator, FormatStrFormatter
from sklearn.preprocessing import PolynomialFeatures
from sklearn.model_selection import train_test_split
from sklearn.linear_model import Lasso
import numpy as np
import numpy.linalg as la
from random import random, seed
import pandas as pd
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error
# Seed parameters
seed = 42069
np.random.seed(seed)
p = 0 # For color cycle


# Franke Function
def FrankeFunction(x,y):
    term1 = 0.75*np.exp(-(0.25*(9*x-2)**2) - 0.25*((9*y-2)**2))
    term2 = 0.75*np.exp(-((9*x+1)**2)/49.0 - 0.1*(9*y+1))
    term3 = 0.5*np.exp(-(9*x-7)**2/4.0 - 0.25*((9*y-3)**2))
    term4 = -0.2*np.exp(-(9*x-4)**2 - (9*y-7)**2)
    return term1 + term2 + term3 + term4

# Creating the design matrix
def design(data, degree):
    poly = PolynomialFeatures(degree=degree) # Takes a set of datapoints and calculates degree 2 polinomial
    X = poly.fit_transform(data) # Only need one dimension?
    return X

# Applying SVD to design matrix A and function b
def svd_solver(A,b):
    U, sigma, VT = la.svd(A)  # Solve SVD system
    Sigma = np.zeros(A.shape) # Initialize empty Sigma array
    diags = np.diag(sigma) # Width of Sigma array
    hehe = len(diags)
    Sigma[:hehe,:hehe] = diags # Fill diagonal elements in top half
    Sigma_pinv = np.zeros(A.shape).T    
    Sigma_pinv[:hehe,:hehe] = np.diag(1/sigma[:hehe])
    Sigma_pinv.round(3)

    x_svd = VT.T.dot(Sigma_pinv).dot(U.T).dot(b) # Solve for x
    return x_svd

def ridge(A,b,lamb):
    x_svd = svd_solver(A,b)
    return (1-lamb)**-1*x_svd

# Mean squared error
def mse(y, y_pred):
    mse = np.mean((y - y_pred)**2)
    return mse

# R-squared
def R_squared(y, y_pred):
    return 1-np.sum( (y - y_pred)**2 )/np.sum( (y - np.mean(y))**2 )

# Bias
def bias(y, y_pred):
    bias = np.mean((y - np.mean(y_pred))**2)
    return  bias

# Variance 
def var2(y_pred):
    variance = np.mean(np.var(y_pred))
    return variance

# Optional plotting step
def plotstuff(split):
    # Plotting parameters
    fig = plt.figure()
    ax1 = fig.add_subplot(111, projection = '3d')
    #ax1.set_zlim3d(-0.2,1.2)
    #ax1.set_zlim3d(-100,400)
    
    # Plot franke function
    surf = ax1.plot_surface(x_, y_, b, alpha=0.5, cmap=cm.coolwarm,label="Franke function") # Plot franke function
    surf._facecolors2d=surf._facecolors3d # Bugfix for legend
    surf._edgecolors2d=surf._edgecolors3d
    
    # Plot training data fit
    ax1.scatter(data_train[0],data_train[1],y_train_pred,alpha=1, s=1, color="C0", label="Training data")
    if split==True:
        # Plot test data fit
        ax1.scatter(data_test[0],data_test[1],y_test_pred,alpha=1, s=1, color="C1", label="Test data")
    plt.legend()
    plt.show()


def error(errors,degrees,plot,filename,lamb,p,printerror):
    if printerror==True:
        for i in range(len(degrees)):
            print(" ")
            print("Polynomial of degree: ", i)
            print("Total errors and STDDEV on training data: ")
            print("MSE: %.5f %.5f,  R^2: %.5f %.5f" %(np.mean(errors[:,0,0,i], axis=0), np.std(errors[:,0,0,i], axis=0), np.mean(errors[:,1,0,i], axis=0), np.std(errors[:,1,0,i], axis=0)))
            
            print("Total errors and STDDEV on test data: ")
            print("MSE: %.5f %.5f,  R^2: %.5f %.5f" %(np.mean(errors[:,0,1,i], axis=0), np.std(errors[:,0,1,i], axis=0), np.mean(errors[:,1,1,i], axis=0), np.std(errors[:,1,1,i], axis=0)))
            
    if plot==True:
        plt.style.use(u"~/.matplotlib/stylelib/trygveplot_astro.mplstyle")
        alpha = 1.0
        # Error
        #plt.plot(degrees, np.mean(errors[:,0,0,:], axis=0),label=r"$\lambda$="+str(lamb)+", Training data",color="C"+str(p)) # Training data
        plt.plot(degrees, np.mean(errors[:,0,1,:], axis=0),label=r"$\lambda$="+str(lamb)+", Test data",linestyle="--",color="C"+str(p),alpha=alpha) # Test data
        
        # Bias
        #plt.plot(degrees, np.mean(errors[:,2,0,:], axis=0),label=r"$\lambda$="+str(lamb)+", Training data bias",color="C"+str(p+1), linestyle=":") # Training data
        plt.plot(degrees, np.mean(errors[:,2,1,:], axis=0),label=r"$\lambda$="+str(lamb)+", Training data bias",color="C"+str(p+1), linestyle=":", alpha=alpha) # Training data
        
        # Variance
        #plt.plot(degrees, np.mean(errors[:,3,0,:], axis=0),label=r"$\lambda$="+str(lamb)+", Test data variance",linestyle="-.",color="C"+str(p+2)) # Test data
        plt.plot(degrees, np.mean(errors[:,3,1,:], axis=0),label=r"$\lambda$="+str(lamb)+", Test data variance",linestyle="-.",color="C"+str(p+2),alpha=alpha) # Test data
        
        # Bias + Variance
        #plt.plot(degrees, np.mean(errors[:,2,0,:], axis=0)+np.mean(errors[:,3,0,:], axis=0),label=r"$\lambda$="+str(lamb)+", Training data",color="C"+str(p+3)) # Training data
        plt.plot(degrees, np.mean(errors[:,2,1,:], axis=0)+np.mean(errors[:,3,1,:], axis=0),label=r"COMBINED",linestyle="--",color="C"+str(p+3),alpha=alpha) # Test data


        
        #plt.title("MSE for training and test data")
        plt.xlabel("Polynomial degree")
        plt.ylabel("Prediction error")
        plt.xlim([0,len(degrees)])
        #plt.ylim([0,0.1])
        plt.legend()

if True:
    # Make data.
    N = 20
    x = np.linspace(0, 1, N)
    y = np.linspace(0, 1, N)

    x_, y_ = np.meshgrid(x,y)
    data = np.c_[(x_.ravel()).T,(y_.ravel()).T]
    X = pd.DataFrame(data)

    # Create and transform franke function data
    noise = 0.1 # Amount of noise
    b = FrankeFunction(x_, y_) + np.random.normal(size=x_.shape)*noise # Franke function with optional gaussian noise
    print(b.shape)
    y = pd.DataFrame(b.ravel().T)
else:
    b = imread("../MachineLearning/doc/Projects/2019/Project1/Datafiles/SRTM_data_Norway_2.tif")[100:120,100:120]
    length = b.shape[0]
    width = b.shape[1]
    x_,y_ = np.meshgrid(range(width), range(length))
    data = np.c_[(x_.ravel()).T,(y_.ravel()).T]
    X = pd.DataFrame(data)
    y = pd.DataFrame(b.ravel().T)

k = 100; split=True
degrees = range(15)
method = "OLS"; lambdas=[0]
#method = "lasso"; lambdas=[0.001, 0.01, 0.1, 0.3]
#method = "ridge"; lambdas=[0.001]

# Something wrong with the bias in OLS? Ridge is weird too.

for lamb in lambdas: # Iterate over all hyperparemeters lambda
    k_errors = np.zeros((k, 4, 2, len(degrees))) # Initializing error array
    d = 0 # Degree index variable
    for degree in degrees: # Iterate over polynomial complexity
        for k in range(k): # K-fold test. Do it k times and check errors.
            if split==True: 
                data_train, data_test, y_train, y_test = train_test_split(X,y,test_size=0.20)
            else:
                data_train = X
                y_train = y
    
            X_train = design(data_train,degree) # Design matrix for training data
            if method=="ridge":
                X_train_reduced = X_train - np.mean(X_train, axis=0) # Zero center training data
                y_train_reduced = y_train - np.mean(y_train, axis=0)
                
                beta = ridge(X_train_reduced,y_train_reduced,lamb)  # Solve for x using ONLY training data

                y_train_pred = X_train_reduced.dot(beta).reshape(y_train_reduced.shape) # Check how beta matches training data                
                y_train_pred += np.mean(y_train, axis=0)[0]
                
            elif method=="lasso":
                fit = Lasso(alpha=lamb).fit(X_train, y_train)
                y_train_pred = fit.predict(X_train).reshape(y_train.shape)

            else: # Regular OLS
                beta = svd_solver(X_train,y_train)  # Solve for x using ONLY training data
                y_train_pred = X_train.dot(beta).reshape(y_train.shape) # Check how beta matches training data

            # Save training errors
            k_errors[k, 0, 0, d] = mse(y_train, y_train_pred)
            k_errors[k, 1, 0, d] = R_squared(y_train, y_train_pred)
            k_errors[k, 2, 0, d] = bias(y_train, y_train_pred)
            k_errors[k, 3, 0, d] = var2(y_train_pred)
            
            if split==True:
                X_test = design(data_test,degree) # Design matrix for test data
                if method=="lasso":
                    y_test_pred = fit.predict(X_test).reshape(y_test.shape)
                elif method=="ridge":
                    X_test_reduced = X_test - np.mean(X_train, axis=0)
                    y_test_pred = X_test_reduced.dot(beta).reshape(y_test.shape) # Test (old) x on test data
                    y_test_pred += np.mean(y_train, axis=0)[0]
                else:
                    y_test_pred = X_test.dot(beta).reshape(y_test.shape) # Test (old) x on test data
                
                # Save test errors
                k_errors[k, 0, 1, d] = mse(y_test, y_test_pred)
                k_errors[k, 1, 1, d] = R_squared(y_test, y_test_pred)
                k_errors[k, 2, 1, d] = bias(y_test, y_test_pred)
                k_errors[k, 3, 1, d] = var2(y_test_pred)

        d += 1 # Index of degree
            
    filename=str(method)+str(lamb)+".png"
    error(k_errors, degrees, True, filename, lamb, p, True)
    p += 1 #Color cycle
plt.show()
#plt.savefig(str(method)+".png")
#plotstuff(split)

