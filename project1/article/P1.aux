\relax 
\providecommand\zref@newlabel[2]{}
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\abx@aux@refcontext{none/global//global/global}
\@writefile{toc}{\boolfalse {citerequest}\boolfalse {citetracker}\boolfalse {pagetracker}\boolfalse {backtracker}\relax }
\@writefile{lof}{\boolfalse {citerequest}\boolfalse {citetracker}\boolfalse {pagetracker}\boolfalse {backtracker}\relax }
\@writefile{lot}{\boolfalse {citerequest}\boolfalse {citetracker}\boolfalse {pagetracker}\boolfalse {backtracker}\relax }
\abx@aux@cite{MHJ}
\abx@aux@segm{0}{0}{MHJ}
\abx@aux@cite{trevor2009elements}
\abx@aux@segm{0}{0}{trevor2009elements}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{2}{section.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2}Regression methods}{2}{section.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Ordinary Least Squares (OLS)}{2}{subsection.2.1}\protected@file@percent }
\newlabel{eq:cost}{{3}{2}{Ordinary Least Squares (OLS)}{equation.2.3}{}}
\newlabel{eq:principal1}{{6}{2}{Ordinary Least Squares (OLS)}{equation.2.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Ridge regression}{2}{subsection.2.2}\protected@file@percent }
\abx@aux@cite{franke}
\abx@aux@segm{0}{0}{franke}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Lasso regression}{3}{subsection.2.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3}Data}{3}{section.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4}The Franke Function}{3}{subsection.3.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5}Terrain data}{3}{subsection.3.5}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4}Resampling and Bias variance}{3}{section.4}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {5}Results}{4}{section.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.6}Franke function data}{4}{subsection.5.6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {}Ordinary Least Squares}{4}{subsubsection.5.6.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {}Ridge regression}{5}{subsubsection.5.6.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {}Lasso regression}{5}{subsubsection.5.6.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.7}Terrain data}{5}{subsection.5.7}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {6}Summary Remarks}{5}{section.6}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces MSE error, bias and variance when using OLS to fit two different datesets with different noise levels. The model is fitted using a polynomial ranging from degree 0 to 15. Not surprisingly, the bias is reduced for the data with lower noise, and we get less overfitting. \relax }}{6}{figure.caption.1}\protected@file@percent }
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {$N(0,0.1)$}}}{6}{subfigure.1.1}\protected@file@percent }
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {$N(0,0.05)$}}}{6}{subfigure.1.2}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:BVols}{{1}{6}{MSE error, bias and variance when using OLS to fit two different datesets with different noise levels. The model is fitted using a polynomial ranging from degree 0 to 15. Not surprisingly, the bias is reduced for the data with lower noise, and we get less overfitting. \relax }{figure.caption.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces The Franke function surface and OLS Regression applied to it with varying degrees of noise. Even with high noise, OLS does a good job of modelling the Franke function.\relax }}{6}{figure.caption.2}\protected@file@percent }
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {Franke function}}}{6}{subfigure.2.1}\protected@file@percent }
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {$N(0,0.05)$}}}{6}{subfigure.2.2}\protected@file@percent }
\@writefile{lof}{\contentsline {subfigure}{\numberline{(c)}{\ignorespaces {$N(0,0.1)$}}}{6}{subfigure.2.3}\protected@file@percent }
\@writefile{lof}{\contentsline {subfigure}{\numberline{(d)}{\ignorespaces {No noise}}}{6}{subfigure.2.4}\protected@file@percent }
\newlabel{fig:OLSfranke}{{2}{6}{The Franke function surface and OLS Regression applied to it with varying degrees of noise. Even with high noise, OLS does a good job of modelling the Franke function.\relax }{figure.caption.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces MSE error, bias and variance when using Ridge and Lasso regression with different $\lambda $ parameters on the Franke function with Gaussian noise $N(0,0.1)$. Plotted using $\lambda = [ 10^{-6}, 10^{-5}, 10^{-4}, 10^{-3}, 10^{-2}]$ over 15 polynomial degrees. \relax }}{8}{figure.caption.3}\protected@file@percent }
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {Ridge regression}}}{8}{subfigure.3.1}\protected@file@percent }
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {Lasso regression}}}{8}{subfigure.3.2}\protected@file@percent }
\newlabel{fig:BVrl}{{3}{8}{MSE error, bias and variance when using Ridge and Lasso regression with different $\lambda $ parameters on the Franke function with Gaussian noise $N(0,0.1)$. Plotted using $\lambda = [ 10^{-6}, 10^{-5}, 10^{-4}, 10^{-3}, 10^{-2}]$ over 15 polynomial degrees. \relax }{figure.caption.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Ridge Regression for four different values of the $\lambda $ parameter. For this analysis, we obtain the best fits with small values of $\lambda $, which brings us closer to OLS.\relax }}{8}{figure.caption.4}\protected@file@percent }
\newlabel{fig:ridgefranke}{{4}{8}{Ridge Regression for four different values of the $\lambda $ parameter. For this analysis, we obtain the best fits with small values of $\lambda $, which brings us closer to OLS.\relax }{figure.caption.4}{}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {Ridge $\lambda =0.1$}}}{8}{subfigure.4.1}\protected@file@percent }
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {Ridge $\lambda =0.01$}}}{8}{subfigure.4.2}\protected@file@percent }
\@writefile{lof}{\contentsline {subfigure}{\numberline{(c)}{\ignorespaces {Ridge $\lambda =0.001$}}}{8}{subfigure.4.3}\protected@file@percent }
\@writefile{lof}{\contentsline {subfigure}{\numberline{(d)}{\ignorespaces {Ridge $\lambda =0.0001$}}}{8}{subfigure.4.4}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Lasso Regression for four values of $\lambda $. Even for small $\lambda $s, the fits are very crude.\relax }}{9}{figure.caption.5}\protected@file@percent }
\newlabel{fig:lassofranke}{{5}{9}{Lasso Regression for four values of $\lambda $. Even for small $\lambda $s, the fits are very crude.\relax }{figure.caption.5}{}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {Lasso $\lambda =0.1$}}}{9}{subfigure.5.1}\protected@file@percent }
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {Lasso $\lambda =0.01$}}}{9}{subfigure.5.2}\protected@file@percent }
\@writefile{lof}{\contentsline {subfigure}{\numberline{(c)}{\ignorespaces {Lasso $\lambda =0.001$}}}{9}{subfigure.5.3}\protected@file@percent }
\@writefile{lof}{\contentsline {subfigure}{\numberline{(d)}{\ignorespaces {Lasso $\lambda =0.0001$}}}{9}{subfigure.5.4}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Values for the 14 first $\beta $-values using a 5th degree polynomial fit for different values of the shrinkage parameter $\lambda $ using Ridge and Lasso regression. Confidence per parameter is calculated across all K-Folds, and plotted as error bars.\relax }}{9}{figure.caption.6}\protected@file@percent }
\newlabel{fig:lambdasridge}{{6}{9}{Values for the 14 first $\beta $-values using a 5th degree polynomial fit for different values of the shrinkage parameter $\lambda $ using Ridge and Lasso regression. Confidence per parameter is calculated across all K-Folds, and plotted as error bars.\relax }{figure.caption.6}{}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {Ridge regression}}}{9}{subfigure.6.1}\protected@file@percent }
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {Ridge regression}}}{9}{subfigure.6.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces The MSE error for the terrain fits. We note how OLS collapses at polynomial degree 8, and is beaten by Ridge regression which carries on to degree 10. Although lasso regression does not collapse in the way Ridge and OLS does, it seems to converge on a higher MSE for very high complexities. \relax }}{10}{figure.caption.7}\protected@file@percent }
\newlabel{fig:BVterrain}{{7}{10}{The MSE error for the terrain fits. We note how OLS collapses at polynomial degree 8, and is beaten by Ridge regression which carries on to degree 10. Although lasso regression does not collapse in the way Ridge and OLS does, it seems to converge on a higher MSE for very high complexities. \relax }{figure.caption.7}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces Polynomial fits on terrain data for a degree 10 polynomial. As we can see, this time, the OLS method collapses above degree 8, while Ridge regression makes a good fit and Lasso struggles to capture the details. \relax }}{10}{figure.caption.8}\protected@file@percent }
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {Terrain data}}}{10}{subfigure.8.1}\protected@file@percent }
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {OLS}}}{10}{subfigure.8.2}\protected@file@percent }
\@writefile{lof}{\contentsline {subfigure}{\numberline{(c)}{\ignorespaces {Ridge $\lambda =10^{-5}$}}}{10}{subfigure.8.3}\protected@file@percent }
\@writefile{lof}{\contentsline {subfigure}{\numberline{(d)}{\ignorespaces {Lasso $\lambda =10^{-5}$}}}{10}{subfigure.8.4}\protected@file@percent }
\newlabel{fig:terrain}{{8}{10}{Polynomial fits on terrain data for a degree 10 polynomial. As we can see, this time, the OLS method collapses above degree 8, while Ridge regression makes a good fit and Lasso struggles to capture the details. \relax }{figure.caption.8}{}}
